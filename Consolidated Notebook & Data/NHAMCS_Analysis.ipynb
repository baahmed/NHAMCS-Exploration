{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NHAMCS-Analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOtwq1M389M1",
        "colab_type": "text"
      },
      "source": [
        "# NHAMCS Dataset:\n",
        "\n",
        "In this notebook, we will be analysing The National Hospital Ambulatory Medical Care Survey (NHAMCS) Dataset. It is a dataset describing Emergency Departments in the US from various different states."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zPVe5GR8_yr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ae1f2071-e981-4c48-919e-497e53c397e4"
      },
      "source": [
        "# The dataset is uploaded on Google Drive so we need to import the drive utility library\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA0uLUwm-aHt",
        "colab_type": "text"
      },
      "source": [
        "The next thing is to import the dataset and inspect it. We will be importing pandas from an SAS file into a pandas Dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYdlkQP4-YG0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "238c039a-7895-4e86-be6b-622a18f16c7a"
      },
      "source": [
        "# import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# read the dataset from the SAS file\n",
        "NHAMCS = pd.read_sas(filepath_or_buffer = '/content/drive/Shared drives/Vodafone Internship/Dataset/ed2017_sas.sas7bdat')\n",
        "\n",
        "# inspect the first few records\n",
        "print(NHAMCS.head())\n",
        "\n",
        "# look at the dimensions of the dataframe\n",
        "\n",
        "print(NHAMCS.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   VMONTH  VDAYR  ARRTIME  WAITTIME  ...     CSTRATM  CPSUM       PATWT      EDWT\n",
            "0     6.0    6.0  b'2056'      72.0  ...  40100000.0    4.0  3723.12641  21.58043\n",
            "1     6.0    2.0  b'1417'      64.0  ...  40100000.0    4.0  3723.12641       NaN\n",
            "2     6.0    2.0  b'2303'      -7.0  ...  40100000.0    4.0  3723.12641       NaN\n",
            "3     6.0    5.0  b'0930'      29.0  ...  40100000.0    4.0  3723.12641       NaN\n",
            "4     6.0    2.0  b'1332'      20.0  ...  40100000.0    4.0  3723.12641       NaN\n",
            "\n",
            "[5 rows x 949 columns]\n",
            "(16709, 949)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHoZ3xfJFSKV",
        "colab_type": "text"
      },
      "source": [
        "The dataframe has about 16.7 thousand examples and each of these has 949 features.\n",
        "\n",
        "## Data Quality\n",
        "\n",
        "In this section, we will be doing 2 things:\n",
        "\n",
        "### 1- Analysing Missing Values\n",
        "Firstly, we will explore the quality of the features present in this dataset. by looking at the percentage of missing values in each of the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf_xiJ1dFWaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bassant's work goes here"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5KekG6uMTzG",
        "colab_type": "text"
      },
      "source": [
        "### 2- Encoding the Categorical Features\n",
        "\n",
        "The second thing in this section is converting the categorical features into a numeric version of them, that could be input to different ML models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eafD-ZFPM56X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hoda's work goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgjvr81IFuKl",
        "colab_type": "text"
      },
      "source": [
        "## Target Label\n",
        "\n",
        "Given the dataset, we need to extract the label that we should be able to predict given the rest of the features. Firstly we wanted to predict the department that each patient should be redirected to. However, we couldn't find any information in the dataset regarding departments.\n",
        "\n",
        "Instead, we decided to go with the immediacy level of each patient. In the dataset, the feature **\"IMMEDR\"** represents exactly that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FntlZ0dqHbQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this would represent the target label that we would be trying to predict\n",
        "\n",
        "immediacyLevel = NHAMCS[['IMMEDR']]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixogm1ydIIGj",
        "colab_type": "text"
      },
      "source": [
        "## Feature Selection\n",
        "\n",
        "In the previous section, we extracted the target feature. As mentioned earlier, we have almost 950 features. Consequently, training a model using all of these features might not be the best idea. Consequently, we have to select some features out of the 950 features.\n",
        "\n",
        "In order to do so, we devised 2 different methods:\n",
        "\n",
        "### 1- Manual Extraction\n",
        "\n",
        "Our first plan was to manually extract the features that 'make sense'. We inspected the textfiles describing the dataset and came up with about 150 features that could be useful. These can be found below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op_zPpPMJogH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The patient ID\n",
        "patID = NHAMCS[['PATCODE']]\n",
        "\n",
        "# Demoggraphics of the patient\n",
        "demographics = NHAMCS[['AGE', 'AGER', 'AGEDAYS', 'SEX', 'PATWT']]\n",
        "\n",
        "# Data related to the ER visit\n",
        "visit = NHAMCS[['WAITTIME', 'PAINSCALE', 'SEEN72', 'TOTDIAG']]\n",
        "\n",
        "# The causes recorded for the patient's situation\n",
        "causes = NHAMCS[['CAUSE1', 'CAUSE2', 'CAUSE3']]\n",
        "\n",
        "# The proposed diagnoses for the patient as well as how probably each of them is\n",
        "diagnoses = NHAMCS[['DIAG1', 'DIAG2', 'DIAG3', 'DIAG4']]\n",
        "diagnosesProbable = NHAMCS[['PRDIAG1', 'PRDIAG2', 'PRDIAG3', 'PRDIAG4']]\n",
        "\n",
        "# The complaints recorded by the patient in their previous visits\n",
        "patientComplaintsDetailed = NHAMCS[['RFV1', 'RFV2', 'RFV3', 'RFV4']]\n",
        "patientComplaintsBroad = NHAMCS[['RFV13D', 'RFV23D', 'RFV33D', 'RFV43D']]\n",
        "\n",
        "# Data related to the patients injury (if any)\n",
        "injuryData = NHAMCS[['INJURY', 'INJPOISAD', 'INJURY72', 'INTENT15', 'INJURY_ENC']]\n",
        "\n",
        "# The patient's vitals\n",
        "vitals = NHAMCS[['VITALSD', 'TEMPDF', 'PULSED', 'RESPRD', 'BPSYSD', 'BPDIASD']]\n",
        "\n",
        "# The patient's disease history\n",
        "previousDiseases = NHAMCS[['ETOHAB' ,'ALZHD','ASTHMA','CANCER','CEBVD','CKD','COPD','CHF','CAD',\n",
        "                           'DEPRN','DIABTYP1','DIABTYP2','DIABTYP0','ESRD','HPE','EDHIV','HYPLIPID','HTN',\n",
        "                           'OBESITY' ,'OSA' ,'OSTPRSIS', 'SUBSTAB', 'NOCHRON','TOTCHRON']]\n",
        "\n",
        "# Blood test results (if any)\n",
        "blood = NHAMCS[['ABG','BAC','BMP','BNP','BUNCREAT','CARDENZ','CBC','CMP','BLOODCX',\n",
        "                'TRTCX','URINECX','WOUNDCX','OTHCX','DDIMER','ELECTROL','GLUCOSE','LACTATE','LFT','PTTINR','OTHERBLD','CARDMON',\n",
        "                'EKG','HIVTEST','FLUTEST','PREGTEST','TOXSCREN','URINE']]\n",
        "\n",
        "# Imaging results (if any)\n",
        "imaging = NHAMCS[['ANYIMAGE','XRAY','CATSCAN','CTCONTRAST','CTAB','CTCHEST','CTHEAD','CTOTHER','CTUNK','MRI','MRICONTRAST','ULTRASND','OTHIMAGE']]\n",
        "\n",
        "# The patient's medicine history\n",
        "medications = NHAMCS[['MED1','MED2','MED3','MED4','MED5','MED6','MED7','MED8','MED9','MED10',\n",
        "                      'MED11','MED12','MED13','MED14','MED15','MED16','MED17','MED18','MED19',\n",
        "                      'MED20','MED21','MED22','MED23','MED24','MED25','MED26','MED27','MED28','MED29','MED30']]\n",
        "\n",
        "# Any medicine prescribed in the ER \n",
        "ERMedications = NHAMCS[['GPMED1','GPMED2','GPMED3','GPMED4','GPMED5','GPMED6','GPMED7','GPMED8','GPMED9','GPMED10',\n",
        "                        'GPMED11','GPMED12','GPMED13','GPMED14','GPMED15','GPMED16','GPMED17','GPMED18','GPMED19',\n",
        "                        'GPMED20','GPMED21','GPMED22','GPMED23','GPMED24','GPMED25','GPMED26','GPMED27','GPMED28','GPMED29','GPMED30']]\n",
        "\n",
        "manually_selected_features = pd.concat([patID, demographics, visit, causes, diagnoses, diagnosesProbable, patientComplaintsDetailed, patientComplaintsBroad, injuryData, \n",
        "                        vitals, previousDiseases, blood, imaging, medications, ERMedications], axis=1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ARGNSqyKqbD",
        "colab_type": "text"
      },
      "source": [
        "### 2- Feature Selection using SK Learn\n",
        "\n",
        "In this section, we will be extracting the features that affect our target the most, using the models from the feature_selection library provided in sklearn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXFiZV6FNmE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hoda's work goes here"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr-FRj6VNnW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}