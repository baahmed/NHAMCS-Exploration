{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EOtwq1M389M1"
   },
   "source": [
    "# NHAMCS Dataset:\n",
    "\n",
    "In this notebook, we will be analysing The National Hospital Ambulatory Medical Care Survey (NHAMCS) Dataset. It is a dataset describing Emergency Departments in the US from various different states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-zPVe5GR8_yr",
    "outputId": "8ead40f5-480e-4286-b89a-491aaaa6f78a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# The dataset is uploaded on Google Drive so we need to import the drive utility library\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EA0uLUwm-aHt"
   },
   "source": [
    "The next thing is to import the dataset and inspect it. We will be importing pandas from an SAS file into a pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "WYdlkQP4-YG0",
    "outputId": "c072a489-5140-4fac-e07d-48df5b8b53f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   VMONTH  VDAYR  ARRTIME  WAITTIME  ...     CSTRATM  CPSUM       PATWT      EDWT\n",
      "0     6.0    6.0  b'2056'      72.0  ...  40100000.0    4.0  3723.12641  21.58043\n",
      "1     6.0    2.0  b'1417'      64.0  ...  40100000.0    4.0  3723.12641       NaN\n",
      "2     6.0    2.0  b'2303'      -7.0  ...  40100000.0    4.0  3723.12641       NaN\n",
      "3     6.0    5.0  b'0930'      29.0  ...  40100000.0    4.0  3723.12641       NaN\n",
      "4     6.0    2.0  b'1332'      20.0  ...  40100000.0    4.0  3723.12641       NaN\n",
      "\n",
      "[5 rows x 949 columns]\n",
      "(16709, 949)\n"
     ]
    }
   ],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# read the dataset from the SAS file\n",
    "NHAMCS = pd.read_sas(filepath_or_buffer = '/content/drive/Shared drives/Vodafone Internship/Dataset/ed2017_sas.sas7bdat')\n",
    "\n",
    "# inspect the first few records\n",
    "print(NHAMCS.head())\n",
    "\n",
    "# look at the dimensions of the dataframe\n",
    "\n",
    "print(NHAMCS.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DHoZ3xfJFSKV"
   },
   "source": [
    "The dataframe has about 16.7 thousand examples and each of these has 949 features.\n",
    "\n",
    "## Data Quality\n",
    "\n",
    "In this section, we will be doing 2 things:\n",
    "\n",
    "### NaNs Exploration\n",
    ">In this subsection of the data quality section, we explore the following:\n",
    "- how many NaNs are there?\n",
    "- how can we visualize them?\n",
    "- what could be the reasons for these NaNs?\n",
    "- further steps to be taken for data quality remarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many NaNs are there?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of NaNs Across all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NHAMCS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cb5c8f2ba9cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNHAMCS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'NHAMCS' is not defined"
     ]
    }
   ],
   "source": [
    "nans = NHAMCS.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bf_xiJ1dFWaq"
   },
   "outputs": [],
   "source": [
    "# Bassant's work goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d5KekG6uMTzG"
   },
   "source": [
    "### 2- Encoding the Categorical Features\n",
    "\n",
    "The second thing in this section is converting the categorical features into a numeric version of them, that could be input to different ML models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eafD-ZFPM56X"
   },
   "outputs": [],
   "source": [
    "# import the LabelEncoder Class\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# creating instance of labelencoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "# Replacing the categorical coloumns with a numerical equivalent\n",
    "NHAMCS['CAUSE1'] = labelencoder.fit_transform(NHAMCS['CAUSE1'])\n",
    "NHAMCS['CAUSE2'] = labelencoder.fit_transform(NHAMCS['CAUSE2'])\n",
    "NHAMCS['CAUSE3'] = labelencoder.fit_transform(NHAMCS['CAUSE3'])\n",
    "NHAMCS['DIAG1'] = labelencoder.fit_transform(NHAMCS['DIAG1'])\n",
    "NHAMCS['DIAG2'] = labelencoder.fit_transform(NHAMCS['DIAG2'])\n",
    "NHAMCS['DIAG3'] = labelencoder.fit_transform(NHAMCS['DIAG3'])\n",
    "NHAMCS['DIAG4'] = labelencoder.fit_transform(NHAMCS['DIAG4'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lgjvr81IFuKl"
   },
   "source": [
    "## Target Label\n",
    "\n",
    "Given the dataset, we need to extract the label that we should be able to predict given the rest of the features. Firstly we wanted to predict the department that each patient should be redirected to. However, we couldn't find any information in the dataset regarding departments.\n",
    "\n",
    "Instead, we decided to go with the immediacy level of each patient. In the dataset, the feature **\"IMMEDR\"** represents exactly that. Howeverm a key requirement has to be met: this feature has to be present in most examples. If this is not the case, then predicting it would be very difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FntlZ0dqHbQC",
    "outputId": "c01bda9a-a24b-43aa-ba43-4f27b27a9ea1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.925608953258724% of the values are missing in the 'IMMEDR' feature\n"
     ]
    }
   ],
   "source": [
    "# Firstly, we will look at the percentage of the missing values (or blank) in the 'IMMEDR' column\n",
    "missingValuesPercentage = (NHAMCS[NHAMCS['IMMEDR']==-9].shape[0] + NHAMCS[NHAMCS['IMMEDR']==-8].shape[0] + NHAMCS[NHAMCS['IMMEDR']==7].shape[0] +NHAMCS[NHAMCS['IMMEDR']==0].shape[0])/16709 *100\n",
    "print(str(missingValuesPercentage) + \"% of the values are missing in the 'IMMEDR' feature\" )\n",
    "\n",
    "# Only 27% is missing and therefore, we will still use it as our target variable\n",
    "immediacyLevel = NHAMCS['IMMEDR']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ixogm1ydIIGj"
   },
   "source": [
    "## Feature Selection\n",
    "\n",
    "In the previous section, we extracted the target feature. As mentioned earlier, we have almost 950 features. Consequently, training a model using all of these features might not be the best idea. Consequently, we have to select some features out of the 950 features.\n",
    "\n",
    "In order to do so, we devised a 2-step process:\n",
    "\n",
    "### 1- Manual Extraction\n",
    "\n",
    "Our first plan was to manually extract the features that 'make sense'. We inspected the textfiles describing the dataset and came up with about 150 features that could be useful. These can be found below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "op_zPpPMJogH"
   },
   "outputs": [],
   "source": [
    "# The patient ID\n",
    "patID = NHAMCS[['PATCODE']]\n",
    "\n",
    "# Demoggraphics of the patient\n",
    "demographics = NHAMCS[['AGE', 'AGER', 'AGEDAYS', 'SEX', 'PATWT']]\n",
    "\n",
    "# Data related to the ER visit\n",
    "visit = NHAMCS[['WAITTIME', 'PAINSCALE', 'SEEN72', 'TOTDIAG']]\n",
    "\n",
    "# The causes recorded for the patient's situation\n",
    "causes = NHAMCS[['CAUSE1', 'CAUSE2', 'CAUSE3']]\n",
    "\n",
    "# The proposed diagnoses for the patient as well as how probably each of them is\n",
    "diagnoses = NHAMCS[['DIAG1', 'DIAG2', 'DIAG3', 'DIAG4']]\n",
    "diagnosesProbable = NHAMCS[['PRDIAG1', 'PRDIAG2', 'PRDIAG3', 'PRDIAG4']]\n",
    "\n",
    "# The complaints recorded by the patient in their previous visits\n",
    "patientComplaintsDetailed = NHAMCS[['RFV1', 'RFV2', 'RFV3', 'RFV4']]\n",
    "patientComplaintsBroad = NHAMCS[['RFV13D', 'RFV23D', 'RFV33D', 'RFV43D']]\n",
    "\n",
    "# Data related to the patients injury (if any)\n",
    "injuryData = NHAMCS[['INJURY', 'INJPOISAD', 'INJURY72', 'INTENT15', 'INJURY_ENC']]\n",
    "\n",
    "# The patient's vitals\n",
    "vitals = NHAMCS[['VITALSD', 'TEMPDF', 'PULSED', 'RESPRD', 'BPSYSD', 'BPDIASD']]\n",
    "\n",
    "# The patient's disease history\n",
    "previousDiseases = NHAMCS[['ETOHAB' ,'ALZHD','ASTHMA','CANCER','CEBVD','CKD','COPD','CHF','CAD',\n",
    "                           'DEPRN','DIABTYP1','DIABTYP2','DIABTYP0','ESRD','HPE','EDHIV','HYPLIPID','HTN',\n",
    "                           'OBESITY' ,'OSA' ,'OSTPRSIS', 'SUBSTAB', 'NOCHRON','TOTCHRON']]\n",
    "\n",
    "# Blood test results (if any)\n",
    "blood = NHAMCS[['ABG','BAC','BMP','BNP','BUNCREAT','CARDENZ','CBC','CMP','BLOODCX',\n",
    "                'TRTCX','URINECX','WOUNDCX','OTHCX','DDIMER','ELECTROL','GLUCOSE','LACTATE','LFT','PTTINR','OTHERBLD','CARDMON',\n",
    "                'EKG','HIVTEST','FLUTEST','PREGTEST','TOXSCREN','URINE']]\n",
    "\n",
    "# Imaging results (if any)\n",
    "imaging = NHAMCS[['ANYIMAGE','XRAY','CATSCAN','CTCONTRAST','CTAB','CTCHEST','CTHEAD','CTOTHER','CTUNK','MRI','MRICONTRAST','ULTRASND','OTHIMAGE']]\n",
    "\n",
    "# The patient's medicine history\n",
    "medications = NHAMCS[['MED1','MED2','MED3','MED4','MED5','MED6','MED7','MED8','MED9','MED10',\n",
    "                      'MED11','MED12','MED13','MED14','MED15','MED16','MED17','MED18','MED19',\n",
    "                      'MED20','MED21','MED22','MED23','MED24','MED25','MED26','MED27','MED28','MED29','MED30']]\n",
    "\n",
    "# Any medicine prescribed in the ER \n",
    "ERMedications = NHAMCS[['GPMED1','GPMED2','GPMED3','GPMED4','GPMED5','GPMED6','GPMED7','GPMED8','GPMED9','GPMED10',\n",
    "                        'GPMED11','GPMED12','GPMED13','GPMED14','GPMED15','GPMED16','GPMED17','GPMED18','GPMED19',\n",
    "                        'GPMED20','GPMED21','GPMED22','GPMED23','GPMED24','GPMED25','GPMED26','GPMED27','GPMED28','GPMED29','GPMED30']]\n",
    "\n",
    "manually_selected_features = pd.concat([patID, demographics, visit, causes, diagnoses, diagnosesProbable, patientComplaintsDetailed, patientComplaintsBroad, injuryData, \n",
    "                        vitals, previousDiseases, blood, imaging, medications, ERMedications], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ARGNSqyKqbD"
   },
   "source": [
    "### 2- Feature Selection using SK Learn\n",
    "\n",
    "After manually extracting about 150 features, we realized that this is still a large value and we decided to use the models from the feature_selection library provided in sklearn to select a small number out of these 150 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "qXFiZV6FNmE0",
    "outputId": "b16fa6e3-65f5-4a01-fb2f-da94786243a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PAINSCALE SEEN72 TOTDIAG BPSYSD BPDIASD CBC CMP OTHERBLD CARDMON EKG\n",
      "0         5      2       0     -9      -9   0   0        0       0   0\n",
      "1        -8      2       1     -9      -9   0   0        0       0   0\n",
      "2        -9      2       0     -9      -9   0   0        0       0   0\n",
      "3        -8      2       0     -9      -9   0   0        0       0   0\n",
      "4        -9      2       1     -9      -9   0   0        0       0   0\n"
     ]
    }
   ],
   "source": [
    "# Import the SelectKBest Class, as well as the f_classif scoring metric\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# create an instance of SelectKBest which will select the best 10 features\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "# Create a new dataframe with the only top 10 features that would affect our label (immediacyLevel)\n",
    "X_new = selector.fit_transform(manually_selected_features, immediacyLevel)\n",
    "\n",
    "# This part extracts the names of the features since the X_new does not contain column names\n",
    "mask = selector.get_support() #list of booleans\n",
    "new_features = [] # The list of your K best features\n",
    "feature_names = manually_selected_features.columns\n",
    "for bool, feature in zip(mask, feature_names):\n",
    "  if bool:\n",
    "    new_features.append(feature)\n",
    "\n",
    "# Replace X_new with itself, along with the names of the columns\n",
    "X_new = pd.DataFrame(X_new, columns=new_features)\n",
    "\n",
    "# print the first 5 records of X_new to inspect it\n",
    "print(X_new.head())\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NHAMCS-Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
