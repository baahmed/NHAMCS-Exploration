{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks For Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "methodology\n",
    ">implement pattern-matching algorithm to group data by same piece of information <br>\n",
    ">see what features to engineer<br>\n",
    ">What features do we need for the immediacy label?<br>\n",
    ">See how to deal with those missing values<br>\n",
    ">feed 5 features at a time to neural network. See the accuracy.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "id": "KnXePibYHVfP",
    "outputId": "4ff5bb1e-c269-45d5-cbb6-4aec095760de"
   },
   "outputs": [],
   "source": [
    "NHAMCS = pd.read_sas(filepath_or_buffer = 'NHAMCS.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NHAMCS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NHAMCS.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaNs Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In this subsection of the data quality section, we explore the following:\n",
    "- how many NaNs are there?\n",
    "- how can we visualize them?\n",
    "- what could be the reasons for these NaNs?\n",
    "- further steps to be taken for data quality remarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many NaNs are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of NaNs Across all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = NHAMCS.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order the series descendingly to be able to see those features with the most NaNs. These will most likely be eliminated from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans.sort_values(ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure all the features are captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans.count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nans "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first entries seem to have ALL NaNs! How prevalent is this? Let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans_max = nans[nans >= int(0.99*16709)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans_max.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "448/949"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "47% of the features are almost all NaNs! Let's continue to investigate different % of missing values. We exclude the ones with almost all NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans_no_max = nans[nans < int(0.99*16709)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans_90 = nans_no_max[nans_no_max >= int((0.90*16709))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nans_90.count())\n",
    "print(104+448, 'features have 90%+ NaNs. That is', 552/949 *100, 'of the features.', 949-448, 'of 949 features have less than 90% NaN.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about those with 70% NaNs? (excluding the ones with 90%+ NaNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans_70 = nans_no_max[nans_no_max > int((0.7*16709))]\n",
    "nans_70.count()\n",
    "print(134+552,'features have 70%+ NaNs. That is', 686/949*100,'of the features.',949-686, 'of 949 features have less than 70%+ NaNs.' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some interesting findings. Let's visualize the NaN trend, descendingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaNs Trend Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans.plot(kind = 'bar', figsize = (10,10), title = 'Number of NaNs per Column', xticks = []);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is so much data unavailable?\n",
    "\n",
    ">From the plot, it seems like we have approximately 550 features whose missing values exceed 80%. Looking at the dataset, this is not exactly surprising. There are four mains reasons. \n",
    "\n",
    "#### 1. Information that could have concisely been represented is represented in multiple variables.\n",
    "\n",
    "\n",
    ">##### EXAMPLE: Payment method. We could use feature engineering to just represent what the source of payment was.\n",
    "><ol>\n",
    "    <li>PAYPRIV = \"Expected source(s) of payment for this visit: Private insurance\"</li>\n",
    "    <li>PAYMCARE =\"Expected source(s) of payment for this visit: Medicare\"</li>\n",
    "    <li>PAYMCAID = \"Expected source(s) of payment for this visit: Medicaid or CHIP or other state-based program\"</li>\n",
    "    <li>PAYWKCMP = \"Expected source(s) of payment for this visit: Workers' compensation\"</li>\n",
    "    <li>PAYSELF = \"Expected source(s) of payment for this visit: Self pay\"</li>\n",
    "    <li>PAYNOCHG = \"Expected source(s) of payment for this visit: No charge/Charity\"</li>\n",
    "    <li>PAYOTH = \"Expected source(s) of payment for this visit: Other\"</li>\n",
    "    <li>PAYDK = \"Expected source(s) of payment for this visit: Unknown\"</li>\n",
    "    <li>PAYTYPER = \"Recoded primary expected source of payment for this visit (based on hierarchy)\"</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "#### 2. Some features, collectively, are a \"list\" for the same piece of information.\n",
    "\n",
    "\n",
    ">##### EXAMPLE: Paitent's medications. Not all pateints are prescribed 30 medicines.\n",
    "><ol>\n",
    "    <li>MED1=\"Medication #1\"</li>\n",
    "    <li>MED2=\"Medication #2\"</li>\n",
    "    <li>MED3=\"Medication #3\"</li>\n",
    "    <li>MED4=\"Medication #4\"</li>\n",
    "    <li>...</li>\n",
    "    <li>MED27=\"Medication #27\"</li>\n",
    "    <li>MED28=\"Medication #28\"</li>\n",
    "    <li>MED29=\"Medication #29\"</li>\n",
    "    <li>MED30=\"Medication #30\"</li>\n",
    "</ol>\n",
    "\n",
    "#### 3. With or without reason 2, a collection of features represent a level of detail for the same information.\n",
    "\n",
    ">##### EXAMPLE: Paitent's complaints. Along with the listing, there is a \"level of detail\" to the complaint.\n",
    "><ol>\n",
    "    <li>RFV1 = \"Patient's complaint, symptom, or other reason for visit #1 - detailed category\"</li>\n",
    "    <li>RFV1 = \"Patient's complaint, symptom, or other reason for visit #2 - detailed category\"</li>\n",
    "    <li>RFV1 = \"Patient's complaint, symptom, or other reason for visit #3 - detailed category\"</li>\n",
    "    <li>RFV1 = \"Patient's complaint, symptom, or other reason for visit #4 - detailed category\"</li>\n",
    "    <li>RFV1 = \"Patient's complaint, symptom, or other reason for visit #5 - detailed category\"</li>\n",
    "    <li>...</li>\n",
    "    <li>RFV13D = \"Patient's complaint, symptom, or other reason for visit #1 - detailed category\"</li>\n",
    "    <li>RFV13D = \"Patient's complaint, symptom, or other reason for visit #2 - detailed category\"</li>\n",
    "    <li>RFV13D = \"Patient's complaint, symptom, or other reason for visit #3 - detailed category\"</li>\n",
    "    <li>RFV13D = \"Patient's complaint, symptom, or other reason for visit #4 - detailed category\"</li>\n",
    "    <li>RFV13D = \"Patient's complaint, symptom, or other reason for visit #5 - detailed category\"</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "#### 4. There are features that are reserved for only special pateints. \n",
    "\n",
    ">##### EXAMPLE: 'AGEDAYS' - age days for babies. This could be missing for the rest of the sample.\n",
    "><ol>\n",
    "    <li>AGE=\"Patient age in years\"</li>\n",
    "    <li>AGER=\"Age recode\"</li>\n",
    "    <li>AGEDAYS=\"Age in days for patients less than one year\"</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### What are some key takeaways from these observations?\n",
    "\n",
    "- All the reasons portray how well-suited this dataset is for a myriad of specific use cases and research, albeit with challenging and elongated data cleaning procedures. \n",
    "\n",
    "- Reasons 1 to 3 Feature engineering may also be used extensively to extract the essence of the required data for the problem at hand.\n",
    "\n",
    "- Reason 4 unleashes an extremely important fact: Do not assume that, if 95% of a column's values are missing, that we could safely drop it. It might be catered for a specific use case and hence missing for the majority. For example, if the use case is to study which diseases result in ER visits for babies the most, although this feature could be mostly missing, it is actually important and the missing % is misleading.\n",
    "\n",
    "- All reasons show us that we might not necessarily need to drop a column with high % of NaNs. From the special features of the available subset along with some domain research, we might be able to replace the missing data accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Comments and Prospective Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values Further Steps\n",
    "- Investigate for also these values: [-9, -8, -7, 0]. -9 always means missing values, so we deal with that as such. However, -8 means \"unknown\" and -7 means \"N/A\". 0 may also mean \"blank.\" For features extracted, according to the nature of each feature, we must decide what to do with entries with those values. Some examples include the below.\n",
    "\n",
    "\t\t\n",
    ">**1. VALUE RESIDF**<br>\n",
    "  >  -9='Blank'<br>\n",
    "   > -8='Unknown'<br>\n",
    "\t1='Private residence'<br>\n",
    "\t2='Nursing home'<br>\n",
    "\t3='Homeless/homeless shelter'<br>\n",
    "\t4='Other'<br>\n",
    "    \n",
    ">**2. VALUE WAITTIMEF**<br>\n",
    "    -9 ='Blank'<br>\n",
    "    -7 ='Not Applicable'<br>\n",
    "    \n",
    ">**3. VALUE PAYTYPERF**<br>\n",
    "      -9 ='All sources of payment are blank'<br>\n",
    "      -8 ='Unknown'<br>\n",
    "       1 ='Private insurance'<br>\n",
    "       2 ='Medicare'<br>\n",
    "       3 ='Medicaid or CHIP or other state-based program'<br>\n",
    "       4 =\"Worker's compensation\"<br>\n",
    "       5 ='Self-pay'<br>\n",
    "       6 ='No charge/Charity'<br>\n",
    "       7 ='Other'   <br>\n",
    " \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "- for the extracted features, based on the problem domain, available data %, and relationships with other features, how can we best replace those missing values? Extensive research is needed.\n",
    "\n",
    "\n",
    "#### Data Quality Concerns For Further Investigation\n",
    "- are the values consistent?\n",
    "- are the types suitable?\n",
    "- is te data too granular or too specific for a use case?\n",
    "- is the data comprehensive for a use case?\n",
    "- invesigate the data collection time and determine its relevance for today.\n",
    "- is the data precise?\n",
    "- are the features selected complete?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MVP: Drop ALL with any NaNs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MVP = NHAMCS.dropna(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MVP.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Feature & Its Meaning\n",
    "\n",
    "IMMEDR=\"Immediacy with which patient should be seen (unimputed)\" => missing values not replaced.\n",
    "\n",
    "-    -9='Blank'\n",
    "-    -8='Unknown'\n",
    "-    0='No triage for this visit but ESA does conduct triage'\n",
    "-\t1='Immediate'\n",
    "-\t2='Emergent'\n",
    "-\t3='Urgent'\n",
    "-\t4='Semi-urgent'\n",
    "-\t5='Nonurgent'\n",
    "-\t7='Visit occurred in ESA that does not conduct nursing triage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MVP['IMMEDR'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MVP['IMMEDR'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MVP['IMMEDR'].hist(bins = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Explore8 = MVP[MVP['IMMEDR']==-8]\n",
    "Explore8['WAITTIME'].hist(bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MVP['WAITTIME'].hist(bins=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Preparing the Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HELPER: Function to Extract the Prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = NHAMCS[['RACEUN','RACER','RACERETH','NOPAY']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTS PASSED\n",
    "\n",
    "#n is the length of the prefix\n",
    "#df is the dataframe\n",
    "def extract_prefixes(n, df):\n",
    "    l = []\n",
    "    for col in df.columns:\n",
    "        l.append((col[:n]))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HELPER: Find the Longest Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTS PASSED \n",
    "def longest_prefix(df):\n",
    "    l = []\n",
    "    for col in df.columns:\n",
    "        l.append(len(col))\n",
    "    return max(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_prefix(MVP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we stop prefixing at 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Prefix frequency per Prefix Length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def prefix_freq(n, df):\n",
    "    l = extract_prefixes(n,df)\n",
    "    counter=Counter(l)\n",
    "    counter = {val[0] : val[1] for val in sorted(counter.items(), key = lambda x: (-x[1], x[0]))}\n",
    "    return counter\n",
    "#result = [list(counter.keys()),list(counter.values())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def prefix_freq_no1(n, df):\n",
    "    final_dict = {}\n",
    "    l = extract_prefixes(n,df)\n",
    "    counter=Counter(l)\n",
    "    \n",
    "    for key in counter:\n",
    "        if (counter[key] != 1):\n",
    "            final_dict[key] = counter[key] \n",
    "            final_dict = {val[0] : val[1] for val in sorted(final_dict.items(), key = lambda x: (-x[1], x[0]))}\n",
    "    return final_dict\n",
    "#result = [list(counter.keys()),list(counter.values())]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return Features with the Same Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_match(prefix, df):\n",
    "    l = []\n",
    "    for col in df.columns:\n",
    "        if(col.startswith(prefix)):\n",
    "            l.append(col)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Features with a Certain Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pref(pref_list, df):\n",
    "    for pref in pref_list:\n",
    "        matching = pattern_match(pref,df)\n",
    "        for col in matching:\n",
    "            df = df.drop([col], axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Investigate Prefixes And Their Meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For a 9-length prefix...\n",
    "\n",
    "- PTONLINEE: these variables discuss a pateint's ability to enter or receive information online. Irrelevant to level of immediacy.\n",
    "- EDISCHSRE: no documentation available. Eliminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefs = list(prefix_freq_no1(9,MVP).keys())\n",
    "print(prefs)\n",
    "MVP = remove_pref(prefs, MVP)\n",
    "print(MVP.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For an 8-length prefix...\n",
    "- ESUMCSRE: not documented. Remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefs = (prefix_freq_no1(8,MVP).keys())\n",
    "print(prefs)\n",
    "MVP = remove_pref(prefs, MVP)\n",
    "print(MVP.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For a 7-length prefix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prefs = (prefix_freq_no1(7,MVP).keys())\n",
    "print(prefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Data Types and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_objects():\n",
    "    types = MVP.columns.to_series().groupby(MVP.dtypes).groups\n",
    "    types_dict = {k.name: v for k, v in types.items()}\n",
    "    print(types_dict.keys())\n",
    "    print(types_dict['object'])\n",
    "    \n",
    "check_objects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which ones are worth keeping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meds = MVP[['MED1', 'MED2', 'MED3', 'MED4', 'MED5', 'MED6',\n",
    "       'MED7', 'MED8', 'MED9', 'MED10', 'MED11', 'MED12', 'MED13', 'MED14',\n",
    "       'MED15', 'MED16', 'MED17', 'MED18', 'MED19', 'MED20', 'MED21', 'MED22',\n",
    "       'MED23', 'MED24', 'MED25', 'MED26', 'MED27', 'MED28', 'MED29', 'MED30']]\n",
    "\n",
    "meds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(30):\n",
    "#    varname = 'MED' + str(i+1)\n",
    "#    MVP[varname] =  MVP[varname].apply(lambda x: int(x.decode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapper = {'M':0,'F':1}\n",
    "#df_appointments.replace({'Gender':mapper}, inplace = True)\n",
    "#df_appointments['DaysLeft'] = timedelta_days_left.apply(lambda x: float(x.days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meds = MVP[['MED1', 'MED2', 'MED3', 'MED4', 'MED5', 'MED6',\n",
    "       'MED7', 'MED8', 'MED9', 'MED10', 'MED11', 'MED12', 'MED13', 'MED14',\n",
    "       'MED15', 'MED16', 'MED17', 'MED18', 'MED19', 'MED20', 'MED21', 'MED22',\n",
    "       'MED23', 'MED24', 'MED25', 'MED26', 'MED27', 'MED28', 'MED29', 'MED30']]\n",
    "\n",
    "meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for med in meds.columns:\n",
    "#    print(meds[meds[med]==-9][med].value_counts()/16709*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(24):\n",
    "#    dropcol = 'MED' + str(i+7)\n",
    "#    MVP.drop(dropcol, axis = 'columns', inplace = True)\n",
    "#MVP.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MED7+, 93%+ is missing: drop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causes = MVP[['CAUSE1', 'CAUSE2', 'CAUSE3']]\n",
    "causes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = MVP[['ARRTIME', 'CAUSE1', 'CAUSE2', 'CAUSE3', 'DIAG1', 'DIAG2', 'DIAG3',\n",
    "       'DIAG4', 'DIAG5', 'HDDIAG1', 'HDDIAG2', 'HDDIAG3', 'HDDIAG4',\n",
    "       'HDDIAG5']]\n",
    "diag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = b'W230'\n",
    "bb.decode(encoding='UTF-8',errors='strict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    varname = 'CAUSE' + str(i+1)\n",
    "    MVP[varname] =  MVP[varname].apply(lambda x: transform_byte(x.decode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Neural Nets to Find The Most Influential Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries for modelling\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential #this is the \"base\" model. It allows us to create any deep learning architecture of our choice by adding layers to it.\n",
    "from tensorflow.keras.layers import Activation, Dense #this helps us to create hidden layers\n",
    "from tensorflow.keras.layers import Dropout #regularization technique to prevent overfitting. It deactivates some neurons at random (weight = 0) to accomplish that.\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(layers, activation):\n",
    "    model = Sequential()\n",
    "    for i, nodes in enumerate(layers):\n",
    "        if i==0:\n",
    "            model.add(Dense(nodes,input_dim=X_train.shape[1]))\n",
    "            model.add(Activation(activation))\n",
    "        else:\n",
    "            model.add(Dense(nodes))\n",
    "            model.add(Activation(activation))\n",
    "    model.add(Dense(1)) # Note: no activation beyond this point\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [(20,), (40, 20), (45, 30, 15)]\n",
    "activations = ['sigmoid', 'relu']\n",
    "param_grid = dict(layers=layers, activation=activations, batch_size = [128, 256], epochs=[30])\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result = grid.fit(X_train_std, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = grid.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "score=accuracy_score(Y_test,y_pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NHAMCS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
